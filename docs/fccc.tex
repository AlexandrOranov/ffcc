\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}

\usepackage{eqnarray}
\usepackage{amsmath}
\usepackage{units}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{stackengine}
\usepackage{xfrac}

\usepackage{multirow}
\usepackage{colortbl}

\allowdisplaybreaks

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator{\atantwo}{atan2}
\DeclareMathOperator{\diag}{diag}


\definecolor{Yellow}{rgb}{1,1, 0.6}
\definecolor{Red}{rgb}{1, 0.6, 0.6}

\newcommand{\todo}[1] {{\color{red}[#1]}}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\binwidth}{h}
\newcommand{\E}[1]{\operatorname{E}\left\lbrack#1\right\rbrack}
\newcommand{\fftv}[1]{\mathcal{F}_\mathrm{v} \left( #1 \right)}
\newcommand{\ifftv}[1]{\mathcal{F}^{-1}_\mathrm{v} \left( #1 \right)}
\newcommand{\lossreg}[1]{\operatorname{reg}\left( #1 \right )}
\newcommand{\lossdata}[1]{f\left( #1 \right )}
% \newcommand{\lossdata}[1]{\operatorname{loss}\left( #1 \right )}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\sixwidth}{1.103in}
\newcommand{\threewidth}{2.22in}

\def\lf{\left\lfloor}
\def\rf{\right\rfloor}

\usepackage{expl3}
\ExplSyntaxOn
\newcommand\latinabbrev[1]{
  \peek_meaning:NTF . {% Same as \@ifnextchar
    #1\@}%
  { \peek_catcode:NTF a {% Check whether next char has same catcode as \'a, i.e., is a letter
      #1.\@ }%
    {#1.\@}}}
\ExplSyntaxOff
\def\etal{\latinabbrev{et al}}

% Include other packages here, before hyperref.

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{287} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Fast Fourier Color Constancy}

\author{
Jonathan T. Barron\\
{\tt\small barron@google.com}
\and
Yun-Ta Tsai\\
{\tt\small yuntatsai@google.com}
}
\maketitle

\begin{abstract}
We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm
which solves illuminant estimation by reducing it to a spatial localization
task on a torus.
By operating in the frequency domain,
FFCC produces lower error rates than the previous state-of-the-art by
$13-20\%$ while being $250-3000\times$ faster.
This unconventional approach introduces challenges regarding aliasing,
directional statistics, and preconditioning, which we address.
By producing a complete posterior distribution over illuminants instead of a
single illuminant estimate, FFCC enables better training techniques,
an effective temporal smoothing technique,
and richer methods for error analysis.
Our implementation of FFCC runs at $\sim 700$ frames per second on a mobile
device, allowing it to be used as an accurate,
real-time, temporally-coherent automatic white balance algorithm.
\end{abstract}

\section{Intro}

A fundamental problem in computer vision is that of estimating the underlying
world that resulted in some observed image \cite{Adelson1996, BarrowTenenbaum78}.
One subset of this problem is color constancy:
estimating the color of the illuminant of the scene and the colors of the objects in the
scene viewed under a white light.
Despite its apparent simplicity, this problem has yielded a great deal
of depth and challenge for both the human vision and computer vision communities
\cite{foster2011color, GijsenijTIP2011}.
Color constancy is also a practical concern in the camera industry:
producing a natural looking photograph without user intervention requires that
the illuminant be automatically estimated and discounted, a process referred to as
``auto white balance'' among practitioners.
Though there is a profound historical connection between color constancy and
consumer photography
(exemplified by Edwin Land, the inventor of both Retinex theory
\cite{land1971lightness} and the Polaroid instant camera),
``color constancy'' and ``white balance'' have come to mean different things
--- color constancy aims to recover the veridical world behind an image,
while white balance aims to give an image a pleasant appearance
consistent with some aesthetic or cultural norm.
But with the current ubiquity of learning-based techniques in computer vision,
both problems reduce to just estimating the
``best'' illuminant from an image, and the question of whether that illuminant
is objectively true or subjectively attractive is just a matter of the
data used during training.

Despite their accuracy, modern learning-based color constancy algorithms are
not immediately suitable as practical white balance algorithms, as practical
white balance has several requirements besides accuracy:
\\ {\bf Speed} -
An algorithm running in a camera's viewfinder must run at $30$ FPS
on mobile hardware.
But a camera's compute budget is precious: demosaicing, face detection,
auto exposure, etc, must also run simultaneously and in real time.
Spending more than a small fraction (say, $5-10\%$) of a camera's
compute budget on white balance is impractical, suggesting that our speed
requirement is closer to $1-5$ milliseconds per frame.
\\ {\bf Impoverished Input} -
Most color constancy algorithms are designed for full resolution, high
bit-depth input images, but operating on such large images is challenging and
costly in practice.
To be fast, the algorithm must work well on the small, low bit-depth
``preview'' images ($32 \times 24$ or $64 \times 48$ pixels, $8$-bit)
which are usually computed by specialized camera hardware for this task.
\\ {\bf Uncertainty} -
In addition to the illuminant, the algorithm should produce
some confidence measure or a complete posterior distribution over
illuminants, thereby enabling convenient downstream integration with
hand-engineered heuristics or external sources of information.
\\ {\bf Temporal Coherence} -
The algorithm should allow the estimated illuminant to be smoothed over time,
to prevent color composition in videos from varying erratically.

In this paper we present a novel color constancy algorithm, which we call
``Fast Fourier Color Constancy'' (FFCC).
Viewed as a color constancy algorithm, FFCC is $13-20\%$ more accurate than the
state of the art on standard benchmarks.
Viewed as a prospective white balance algorithm, FFCC addresses our
previously described requirements: Our technique is
$250-3000\times$ faster than the state of the art, and is capable of running at
$1.44$ milliseconds per frame on a standard consumer mobile platform using the
thumbnail images already produced by that camera's hardware.
FFCC produces a complete posterior distribution over illuminants which allows
us to reason about uncertainty and enables simple and effective temporal smoothing.

We build on the ``Convolutional Color Constancy`` (CCC) approach
of \cite{BarronICCV2015}, which is
currently one of the top-performing techniques on standard color constancy
benchmarks \cite{Cheng14,Gehler08,shifunt}.
CCC works by observing that applying a per-channel gain to a linear RGB
image is equivalent to inducing a 2D translation of the log-chroma histogram
of that image, which allows color constancy to be reduced to the
task of localizing a signature in log-chroma histogram space.
This reduction is at the core of the success of CCC and, by extension,
our FFCC technique;
see \cite{BarronICCV2015} for a thorough explanation.
The primary difference between FFCC is that instead of performing an expensive
localization on a large log-chroma plane, we perform a cheap
localization on a small log-chroma \emph{torus}.

\begin{figure}[!]
\centering
  \stackunder[5pt]
    {\includegraphics[width=1.55in]{figures/face1.png}}
    {(a) Image $A$}
  \stackunder[5pt]
    {\includegraphics[width=1.55in]{figures/face2_rep.png}}
    {(b) Aliased Image $B$}
  \caption{
  CCC \cite{BarronICCV2015} reduces color constancy
  to a 2D localization problem similar to object detection (1a).
  FFCC repeatedly wraps this 2D localization problem around a small torus (1b),
  which creates challenges but allows for faster illuminant estimation.
  See the text for details.
  \label{fig:face}
  }
\end{figure}

\newcommand{\mypm}{\mathbin{\smash{%
\raisebox{0.35ex}{%
            $\underset{\raisebox{0.5ex}{$\smash -$}}{\smash+}$%
            }%
        }%
    }%
}

At a high level, CCC reduces color constancy to object detection ---
in the computability theory sense of ``reduce''.
FFCC reduces color constancy to localization on a torus instead of a plane,
and because this task has no intuitive analogue in computer vision we will
attempt to provide one\footnote{
We cannot speak to the merit of this idea in the context of object detection,
and we present it here solely to provide an intuition of our work on color
constancy}.
Given a large image $A$ on which we would like to perform object detection,
imagine constructing a smaller $n \times n$ image $B$ in which each pixel in $B$
is the sum of all values in $A$ separated by a multiple of $n$ pixels in either
dimension:
\begin{equation}
B(i,j) = \sum_{k, l} A(i + nk, j + nl)
\end{equation}
This amounts to taking $A$ and repeatedly wrapping it around a small torus
(see Figure~\ref{fig:face}).
Detecting objects this way may yield a speedup as the image being
processed is smaller, but it also raises new problems:
1) pixel values are corrupted with superimposed shapes that make detection difficult,
2) detections must ``wrap'' around the edges of this toroidal image,
and 3) instead of an absolute, global location we can only recover an aliased,
incomplete location.
FFCC works by taking the large convolutional problem of CCC
(ie, face detection on $A$)
and aliasing that problem down to a smaller size where it can be solved efficiently
(ie, face detection on $B$).
We will show that we can learn an effective color constancy model in the face of
the difficulty and ambiguity introduced by aliasing.
This convolutional classifier will be implemented and learned using FFTs,
because the naturally periodic nature of FFT convolutions resolves
the problem of detections ``wrapping'' around the edge of toroidal images,
and produces a significant speedup.

Our approach to color constancy introduces a number of issues.
The aforementioned periodic ambiguity resulting from operating
on a torus (which we dub ``illuminant aliasing'') requires new techniques
for recovering a global illuminant estimate from an aliased estimate
(Section~\ref{sec:aliasing}).
Localizing the centroid of the illuminant on a torus is difficult, requiring
that we adopt and extend techniques from the directional statistics literature
(Section~\ref{sec:bvm}).
But our approach presents a number of benefits.
FFCC improves accuracy relative to CCC by $17-24\%$ while retaining its flexibility,
and allows us to construct priors over illuminants (Section~\ref{sec:extensions}).
By learning in the frequency-domain we can
construct a novel method for fast frequency-domain regularization and
preconditioning, making FFCC training $20\times$ faster than CCC
(Section~\ref{sec:fourier}).
Our model produces a complete unimodal posterior over illuminants as output,
allowing us to construct a Kalman filter-like approach for processing videos
instead of independent images
(Section~\ref{sec:temporal}).

\begin{figure*}[!]
\centering
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_input.png}}
    {\footnotesize (a) Input Image}
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_histogram_big.png}}
    {\footnotesize (b) Histogram}
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_histogram_aliased.png}}
    {\footnotesize (c) Aliased Histogram}
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_prediction_aliased.png}}
    {\footnotesize (d) Aliased Prediction}
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_prediction_dealiased_anno.png}}
    {\footnotesize (e) De-aliased Prediction}
  \stackunder[5pt]
    {\includegraphics[width=\sixwidth]{figures/overview_prediction_output.png}}
    {\footnotesize (f) Output Image}
    \caption{
    An overview of our pipeline demonstrating the problem of illuminant
    aliasing.
    Similarly to CCC, we take an input image (2a) and
    transform it into a log-chroma histogram (2b, presented
    in the same format as in \cite{BarronICCV2015}).
    But unlike CCC, our histograms are small and toroidal, meaning that pixels
    can ``wrap around'' the edges (2c, with the torus
    ``unwrapped'' once in every direction). This means that the
    centroid of a filtered histogram, which would simply be \emph{the} illuminant
    estimate in CCC, is instead an infinite family of possible illuminants (2d).
    This requires \emph{de-aliasing}, some technique for disambiguating between
    illuminants to select the single most likely estimate
    (2e, shown as a point surrounded by an ellipse visualizing
    the output covariance of our model).
    Our model's output $(u, v)$ coordinates in this de-aliased log-chroma space
    corresponds to the color of the illuminant, which can then be divided
    into the input image to produce a white balanced image (2f).
    \label{fig:overview}
    }
\end{figure*}

\section{Convolutional Color Constancy}
\label{sec:ccc}

Let us review the assumptions made in CCC and inherited by our model.
Assume that we have a photometrically linear input image $I$ from a camera,
with a black level of zero and with no saturated
pixels\footnote{in practice, saturated pixels are identified and removed from all
downstream computation, similarly to how color checker pixels are ignored.}.
Each pixel $k$'s RGB value in image $I$ is assumed to be the product of that
pixel's ``true'' white-balanced RGB value $W^{(k)}$
and some global RGB illumination $L$ shared by all pixels:
\begin{align}
\forall_k \,\,
\begin{bmatrix} I_r^{(k)} \\ I_g^{(k)} \\ I_b^{(k)} \end{bmatrix} = \begin{bmatrix} W_r^{(k)} \\ W_g^{(k)} \\ W_b^{(k)} \end{bmatrix} \circ \begin{bmatrix} L_r \\ L_g \\ L_b \end{bmatrix}
\label{eq:image_formation_rgb}
\end{align}
The task of color constancy is to use the input image $I$ to estimate $L$, and
with that produce $W^{(k)} = I^{(k)} / L$.

Given a pixel from our input RGB image $I^{(k)}$, CCC defines two log-chroma
measures:
\begin{align}
u^{(k)} &= \log \left( I^{(k)}_g / I^{(k)}_r \right) & v^{(k)} &= \log \left( I^{(k)}_g / I^{(k)}_b \right)
\end{align}
The absolute scale of $L$ is assumed to be unrecoverable,
so estimating $L$ simply requires estimating its log-chroma:
\begin{align}
L_u &= \log \left( L_g / L_r \right) & L_v &= \log \left( L_g / L_b \right)
\end{align}
After recovering $(L_u, L_v)$, assuming that $L$ has a magnitude of $1$ lets us
recover the RGB values of the illuminant:
\begin{align}
L_r &= {\exp(-L_u) \over z} \qquad L_g = {1 \over z} \qquad L_b = {\exp(-L_v) \over z} \nonumber \\
z &= \sqrt{\exp(-L_u)^2 + \exp(-L_v)^2 + 1}  \label{eq:light_rgb}
\end{align}
Framing color constancy in terms of predicting log-chroma has several small
advantages over the standard RGB approach ($2$ unknowns instead of $3$, better
numerical stability, etc) but the primary advantage of this approach is that using
log-chroma turns the multiplicative constraint relating $W$ and $I$ into
an additive constraint \cite{finlayson2001color}, and this in turn enables a
convolutional approach to color constancy.
As shown in \cite{BarronICCV2015}, color constancy can be framed as a 2D
spatial localization task on a log-chroma histogram $N$, where some
sliding-window classifier is used to filter that histogram and the centroid of
that filtered histogram is used as the log-chroma of the illuminant.

\section{Illuminant Aliasing}
\label{sec:aliasing}

We assume the same convolutional premise of CCC, but with one primary
difference to improve quality and speed: we use FFTs to perform the convolution
that filters the log-chroma histogram, and we use a small histogram
to make that convolution as fast as possible. This change may seem trivial,
but the periodic nature of FFT convolution combined with the properties of
natural images has a significant effect, as we will demonstrate.

Similarly to CCC, given an input image $I$ we construct a histogram $N$ from $I$,
where $N(i,j)$ is the number of pixels in $I$ whose log-chroma is near the
$(u, v)$ coordinates corresponding to histogram position $(i, j)$:
\begin{align}
N(i, j) = \displaystyle \sum_k \Bigg( & \operatorname{mod} \left( { u^{(k)} - u_\mathit{lo} \over \binwidth} - i, n \right) < 1 \nonumber \\
\wedge \,\, & \operatorname{mod} \left( { v^{(k)} - v_\mathit{lo} \over \binwidth} - j, n \right) < 1 \Bigg) \label{eq:Nhist}
\end{align}
Where $i$, $j$ are $0$-indexed,
$n = 64$ is the number of bins,
$\binwidth = 1/32$ is the bin size, and
$(u_{\mathit{lo}}, v_{\mathit{lo}})$ is the starting point of the histogram.
Because our histogram is too small to contain the wide spread of colors present
in most natural images,
we use modular arithmetic to cause pixels to ``wrap around'' with respect to log-chroma
(any other standard boundary condition would violate our convolutional
assumption and would cause many image pixels to be ignored).
This means that, unlike standard CCC,
a single $(i,j)$ coordinate in the histogram no longer corresponds to
an absolute $(u,v)$ color, but instead corresponds to an infinite family of
$(u,v)$ colors.
Accordingly, the
centroid of a filtered histogram no longer corresponds to the color of the
illuminant, but instead is an infinite set of illuminants.
We will refer to this phenomenon as \emph{illuminant aliasing}.
Solving this problem requires that we use some technique to de-alias an aliased
illuminant estimate\footnote{It is tempting to refer to resolving the illuminant
aliasing problem as ``anti-aliasing'', but anti-aliasing usually refers to
preprocessing a signal to prevent aliasing during some resampling operation,
which does not appear possible in our framework.
``De-aliasing'' suggests that we allow aliasing to happen to the input, but then
remove the aliasing from the output.}.
A high-level outline of our FFCC pipeline that illustrates illuminant (de-)aliasing
can be seen in Fig.~\ref{fig:overview}.

De-aliasing requires that we use some external information (or some external
color constancy algorithm) to disambiguate between illuminants.
An intuitive approach is to select the illuminant
that causes the average image color to be as neutral as possible,
which we call ``gray world de-aliasing''.
We compute average log-chroma values $(\bar{u}, \bar{v})$ for the entire image
and use this to turn an aliased illuminant estimate $(\hat{L}_u, \hat{L}_v)$
into a de-aliased illuminant $(\hat{L}'_u, \hat{L}'_v)$:
\begin{align}
\bar{u} = \log \left( \textstyle \sum_k u^{(k)} \right) \quad
\bar{v} = \log \left( \textstyle \sum_k v^{(k)} \right) \\
\begin{bmatrix} \hat{L}'_u \\ \hat{L}'_v \end{bmatrix} = \begin{bmatrix} \hat{L}_u \\ \hat{L}_v \end{bmatrix} - (n \binwidth) \floor{{1 \over n \binwidth} \begin{bmatrix} \hat{L}_u - \bar{u} \\ \hat{L}_v - \bar{v} \end{bmatrix} + {1 \over 2} } % rounding towards the nearest integer
\label{eq:grayworld}
\end{align}
Another approach, which we call ``gray light de-aliasing'', is to assume
that the illuminant is as close to the center of the histogram as possible.
This de-aliasing approach simply requires carefully setting the starting point
of the histogram $(u_{\mathit{lo}}, v_{\mathit{lo}})$ such that the true
illuminants in natural scenes all lie within the span of the histogram, and
setting $\hat{L}' = \hat{L}$.
We do this by setting $u_{\mathit{lo}}$ and $v_{\mathit{lo}}$
to maximize the distance between the edges
of the histogram and the bounding box surrounding the ground-truth illuminants
in the training data\footnote{Our histograms are shifted toward green colors
rather than centered around a neutral color, as cameras are traditionally
designed with an more sensitive green channel which enables white balance to be
performed by gaining red and blue up without causing color clipping. Ignoring
this practical issue, our approach can be thought of as centering our histograms
around a neutral white light}.
Gray light de-aliasing is trivial to implement but,
 unlike gray world de-aliasing, it will systematically
fail if the histogram is too small to fit all illuminants within its span.

To summarize the difference between CCC \cite{BarronICCV2015} and our approach
with regards to illuminant aliasing, CCC (approximately) performs illuminant
estimation as follows:
\begin{equation}
\begin{bmatrix} \hat{L}_u \\ \hat{L}_v \end{bmatrix} = \begin{bmatrix}u_\mathit{lo} \\ v_\mathit{lo}\end{bmatrix} + \binwidth \left( \argmax_{i, j}\left( N * F \right) \right)
\end{equation}
Where $N * F$ is performed using a pyramid convolution.
FFCC corresponds to this procedure:
\begin{align}
P &\leftarrow \operatorname{softmax} \left( N * F \right) \label{eq:softmax1} \\
(\boldsymbol{\mu}, \boldsymbol{\Sigma}) &\leftarrow \operatorname{fit\_bvm}(P)  \label{eq:bvm1} \\
\begin{bmatrix} \hat{L}_u \\ \hat{L}_v \end{bmatrix} &\leftarrow \operatorname{de\_alias}(\boldsymbol{\mu}) \label{eq:dealias}
\end{align}
Where $N$ is a small and aliased toroidal histogram, convolution is performed with
FFTs, and the centroid of the filtered histogram is estimated and
de-aliased as necessary.
By constructing this pipeline to be differentiable we can train our model in an end-to-end fashion by propagating the gradients of some loss computed on the
de-aliased illuminant prediction $\hat{L}$ back onto the learned filters $F$.
The centroid fitting in Eq.~\ref{eq:bvm1} is performed by fitting
a bivariate von Mises distribution to a PDF, which we will now explain.

\section{Differentiable Bivariate von Mises}
\label{sec:bvm}

Our architecture requires some mechanism for reducing a
toroidal PDF $P(i,j)$ to a single estimate of the illuminant.
Localizing the center of mass of a histogram defined on a torus is difficult:
fitting a bivariate Gaussian may
fail when the input distribution ``wraps around'' the sides of the PDF, as
shown in Fig.~\ref{fig:vonmises}.
Additionally, for the sake of temporal smoothing (Section~\ref{sec:temporal})
and confidence estimation,
we want our model to predict a well-calibrated covariance matrix around the
center of mass of $P$.
This requires that our model be trained end-to-end,
which therefore requires that our mean/covariance fitting be analytically
differentiable and therefore usable as a ``layer'' in our learning
architecture.
To address these problems we present a variant of the bivariate von Mises
distribution~\cite{Mardia1975}, which we will use to efficiently localize
the mean and covariance of $P$ in a manner that allows for easy backpropagation.

The bivariate von Mises distribution (BVM) is a common parameterization of a
PDF on a torus.
There exist several parametrizations
which mostly differ in how ``concentration'' is represented
(``concentration'' having a similar meaning to covariance).
All of these parametrizations present problems in our use case:
none have closed form expressions for maximum likelihood estimators \cite{Hamelryck2012},
none lend themselves to convenient backpropagation,
and all define concentration in terms of angles and therefore
require ``conversion'' to covariance matrices during color de-aliasing.
For these reasons we present an alternative parametrization in which we directly
estimate a BVM as a mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$
in a simple and differentiable closed form expression.
Though necessarily approximate,
our estimator is accurate when the distribution is well-concentrated,
which is generally the case for our task.

\begin{figure}[!]
\centering
  \includegraphics[width=1.05in]{figures/vonmises/0047.png}
  \includegraphics[width=1.05in]{figures/vonmises/0029.png}
  \includegraphics[width=1.05in]{figures/vonmises/0042.png}
  \caption{
  We fit a bivariate von Mises distribution (shown in
  solid blue) to toroidal PDFs $P(i,j)$ to produce an aliased illuminant estimate.
  Contrast this with fitting a bivariate Gaussian (shown in dashed red)
  which treats the PDF as if it lies on a plane.
  Both approaches behave similarly if the distribution lies near the center of the
  unwrapped plane (left) but fitting a Gaussian fails as the distribution begins
  to ``wrap around'' the edge (middle, right).
  \label{fig:vonmises}
  }
\end{figure}

Our input is a PDF $P(i,j)$ of size $n \times n$,
where $i$ and $j$ are integers in $[0, n-1]$.
For convenience we define a mapping from $i$ or $j$ to angles in $[ 0, 2\pi )$
and the marginal distributions of $P$ with respect to $i$ and $j$:
\begin{equation}
\theta(i) = {2\pi i \over n} \quad\,\, P_i(i) = \sum_j P(i, j) \quad\,\, P_j(j) = \sum_i P(i, j) \nonumber
\end{equation}
We also define the marginal expectation of the sine and cosine of the angle:
\begin{equation}
y_i = \sum_i P_i(i) \sin(\theta(i)) \quad x_i = \sum_i P_i(i) \cos(\theta(i))
\end{equation}
With $x_j$ and $y_j$ defined similarly.

Estimating the mean $\boldsymbol \mu$ of a BVM from a histogram just requires
computing the circular mean in $i$ and $j$:
\begin{equation}
\boldsymbol{\mu} = \begin{bmatrix}u_\mathit{lo} \\ v_\mathit{lo}\end{bmatrix} + \binwidth\begin{bmatrix}
\operatorname{mod}\left( {n \over 2\pi} \atantwo (y_i, x_i), n \right) \\
\operatorname{mod}\left( {n \over 2\pi} \atantwo (y_j, x_j), n \right) \\
\end{bmatrix} \label{eq:mu}
\end{equation}
Eq.~\ref{eq:mu} includes gray light de-aliasing, though gray world de-aliasing can also
be applied to $\boldsymbol{\mu}$ after fitting.

\newcommand{\ishift}{\bar{i}}
\newcommand{\jshift}{\bar{j}}

We can fit the covariance of our model
by simply ``unwrapping'' the coordinates of the histogram relative to the
estimated mean and treating these unwrapped coordinates
as though we are fitting a bivariate Gaussian.
We define the ``unwrapped'' $(i, j)$ coordinates such that the ``wrap around''
point on the torus lies as far away from the mean as possible,
or equivalently,
such that the unwrapped coordinates are as close to the mean as possible:
\begin{align}
\ishift & = \operatorname{mod}\left( i - \floor{\boldsymbol{\mu}_u - u_\mathit{lo} \over h} + {n \over 2}, n \right) \nonumber \\
\jshift & = \operatorname{mod}\left( j - \floor{\boldsymbol{\mu}_v - v_\mathit{lo} \over h} + {n \over 2}, n \right)
\end{align}
Our estimated covariance matrix is simply the sample covariance of $P(\ishift, \jshift)$:
\begin{equation}
\displaystyle \E{\ishift} = \sum_i P_i(i) \ishift \quad \quad \displaystyle \E{\jshift} = \sum_j P_j(j) \jshift
\end{equation}
\begin{equation}
\resizebox{2.9in}{!}{$
\boldsymbol{\Sigma} = h^2
\begin{bmatrix}
\displaystyle \epsilon + \sum_i P_i(i) \ishift^2 - \E{\ishift}^2&
\displaystyle \sum_{i,j} P(i,j) \ishift \jshift - \E{\ishift} \E{\jshift} \\
\displaystyle \sum_{i,j} P(i,j) \ishift \jshift - \E{\ishift} \E{\jshift} &
\displaystyle \epsilon + \sum_j P_j(j) \jshift^2 - \E{\jshift}^2
\end{bmatrix}
$} \label{eq:covariance}
\end{equation}
We regularize the sample covariance matrix slightly by adding a constant $\epsilon=1$
to the diagonal.

With our estimated mean and covariance we can compute our loss: the negative log-likelihood
of a Gaussian (ignoring scale factors and constants) relative to the true illuminant $L^*$:
\begin{equation}
\resizebox{2.9in}{!}{$
\lossdata{\boldsymbol{\mu}, \boldsymbol{\Sigma}} = \log{| \boldsymbol{\Sigma} | } + \left( \begin{bmatrix}
L^*_u \\ L^*_v
\end{bmatrix} - \boldsymbol{\mu} \right)^\mathrm{T} \boldsymbol{\Sigma}^{-1} \left( \begin{bmatrix}
L^*_u \\ L^*_v
\end{bmatrix} - \boldsymbol{\mu} \right) \label{eq:lossdata}
$}
\end{equation}
Using this loss causes our model to produce a well-calibrated complete posterior
of the illuminant instead of just a single estimate.
This posterior will be useful when processing video sequences
(Section~\ref{sec:temporal})
and also allows us to attach confidence estimates to our predictions
using the entropy of $\boldsymbol{\Sigma}$ (see the supplement).

Our entire system is trained end-to-end, which requires
that every step in BVM fitting and loss computation be
analytically differentiable. See the supplement for the analytical gradients
for Eqs.~\ref{eq:mu}, \ref{eq:covariance}, and \ref{eq:lossdata}, which
can be chained together to backpropagate the gradient of $\lossdata{\cdot}$
onto the input PDF $P$.

\section{Model Extensions}
\label{sec:extensions}

The system we have described thus far (compute a periodic histogram of
each pixel's log-chroma, apply a learned FFT convolution, apply a softmax,
fit a de-aliased bivariate von Mises distribution) works reasonably well
(Model A in Table~\ref{table:gehler_shi})
but does not produce state-of-the-art results.
This is likely because this model reasons about pixels independently,
ignores all spatial information in the image,
and does not consider the absolute color of the illuminant.
Here we present extensions to the model which address these issues
and improve accuracy accordingly.

As explored in \cite{BarronICCV2015}, a CCC-like model
can be generalized to a set of ``augmented'' images provided that these images
are non-negative and ``scale with intensity'' \cite{Finlayson2013}.
This lets us apply certain filtering operations to image $I$ and, instead of
constructing a single histogram from our image,
construct a ``stack'' of histograms constructed from the image and its
filtered versions. Instead of learning
and applying one filter, we learn a stack of filters and sum across
channels after convolution.
The general family of augmented images used in \cite{BarronICCV2015}
are expensive to compute, so we instead use just
the input image $I$ and a local measure of absolute deviation in the input image:
\begin{equation}
\resizebox{2.9in}{!}{$
E(x,y,c) = {1 \over 8} \displaystyle \sum_{i = -1}^1 \sum_{j = -1}^1 \abs{I(x,y,c) - I(x+i,y+j,c)}
$}
\end{equation}
These two features appears to perform similarly to the four features used in
\cite{BarronICCV2015}, while being cheaper to compute.

Just as a sliding-window object detector is often invariant to the absolute location of an
object in an image,
the convolutional nature of our baseline model makes it invariant to any global shift of the color of the input image.
This means that our baseline model \emph{cannot} rely on any statistical
regularities of the illumination by, say,
modeling black body radiation,
the specific properties of commonly manufactured light bulbs,
or any varying spectral sensitivity across cameras.
Though CCC does not model illumination directly,
it appears to indirectly reason about illumination by using the boundary
conditions of its pyramid convolution to learn a model which is not truly
spatially varying and is therefore sensitive to absolute color.
Because a torus has no boundaries, our model is invariant to
global input color, so we must therefore
introduce a mechanism for directly reasoning about illuminants.
We use a per-illuminant ``gain'' map $G(i,j)$ and ``bias'' map $B(i,j)$,
which together apply a per-illuminant affine transformation to the output of our
previously-described convolution at (aliased) color $(i,j)$.
The bias $B$ causes our model
to prefer certain illuminants over others, while the gain $G$ causes the
contribution of the convolution at certain colors to be amplified.

Our two extensions (an augmented edge channel and an illuminant gain/bias map)
let us redefine the $P$ in Eq.~\ref{eq:softmax1} as
\begin{equation}
P = \operatorname{softmax} \left(B + G \circ \sum_k \left( N_k * F_k \right) \right)
\end{equation}
Where $\{F_k\}$ are the set of learned filters for each augmented channel's
histogram $N_k$,
$G$ is our learned gain map, and $B$ is our learned bias map.
In practice we actually parametrize $G_{\log}$ when training and define
$G = \exp(G_{\log})$, which constraints $G$ to be non-negative.
Visualizations of $G$ and $B$ and our learned filters can be seen in
Fig.~\ref{fig:blackbody}.


\begin{figure}[!]
\centering
  \stackunder[5pt]
    {\includegraphics[width=0.78in]{figures/filter_pixel.png}}
    {\footnotesize (a) Pixel Filter \label{fig:filter_pixel}}
  \stackunder[5pt]
    {\includegraphics[width=0.78in]{figures/filter_edge.png}}
    {\footnotesize (b) Edge Filter \label{fig:filter_edge}}
  \stackunder[5pt]
    {\includegraphics[width=0.78in]{figures/blackbody_gain.png}}
    {\footnotesize (c) Illum. Gain \label{fig:blackbody_gain}}
  \stackunder[5pt]
    {\includegraphics[width=0.78in]{figures/blackbody_bias.png}}
    {\footnotesize (d) Illum. Bias \label{fig:blackbody_bias}}
  \caption{
  A complete learned model (Model J in Table~\ref{table:gehler_shi})
  shown in centered $(u,v)$ log-chroma space, with brightness indicating
  larger values.
  Our learned filters are centered around the origin
  (the predicted white point)
  and our illuminant gain and bias maps model the black body curve and
  varying camera sensitivity as two wrap-around line segments (this dataset
  consists of images from two different cameras).
  \label{fig:blackbody}
  }
\end{figure}

\section{Fourier Regularization and Preconditioning}
\label{sec:fourier}

Our learned model weights $( \{ F_k \}, G, B )$ are all periodic $n \times n$ images.
To improve generalization, we want these weights to be small and smooth.
In this section we present the general form of the regularization used during
training, and we show how this regularization lets us precondition the
optimization problem solved during training to find lower-cost minima in fewer
iterations.
Because this frequency-domain optimization technique applies
generally to any optimization problem concerning smooth and periodic images,
we will describe it in general terms.

Let us construct an optimization problem with respect to a single $n \times n$
image $Z$ consisting of a data term $f(Z)$ and a regularization term $g(Z)$:
\begin{equation}
Z^* = \argmin_Z \left( f\left(Z\right) + g\left(Z\right) \right)
\label{eq:primal_loss}
\end{equation}
We require that the regularization $g(Z)$
is the weighted sum of squared periodic convolutions of $Z$ with some filter bank.
In our experiments $g(Z)$ is the weighted
sum of the squared difference between adjacent values (similar to a total
variation loss \cite{Rudin1992}) and the sum of squared values:
\begin{align}
g(Z) = \textstyle & \lambda_1 \textstyle \sum_{i, j} \big( \left(Z \left(i,j \right) - Z \left(\operatorname{mod}(i+1, n),j \right) \right)^2 \nonumber \\
 & \quad\quad\,\,\,\, + \left( Z\left(i,j\right) - Z\left(i,\operatorname{mod}(j+1, n)\right) \right)^2 \big) \nonumber \\
 + & \lambda_0 \textstyle \sum_{i, j} Z(i,j)^2 \label{eq:reg}
\end{align}
Where $\lambda_1$ and $\lambda_0$ are hyperparameters that determine the
strength of each smoothness term. We require that $\lambda_0 > 0$ to prevent
divide-by-zero issues during preconditioning.

We use a variant of the standard FFT
$\fftv{\cdot}$ which bijectively maps from some real $n \times n$ image to a
real $n^2$-dimensional vector, instead of the complex $n \times n$ image
produced by a standard FFT (See the supplement for a formal description).
With this, we can rewrite Eq.~\ref{eq:reg} as follows:
\begin{align}
&\mathbf{w} = {1 \over n} \sqrt{ \lambda_1 \left( \abs{ \fftv{[1, -1]}}^2 + \abs{\fftv{[1; -1]}}^2 \right) + \lambda_0} \nonumber \\
&g(Z) = \fftv{Z}^\mathrm{T} \diag \left( \mathbf{w} \right)^2  \fftv{Z}
\end{align}
where the vector $\mathbf{w}$ is just some fixed function of the definition
of $g(Z)$ and the values of the hyperparameters $\lambda_1$ and $\lambda_0$.
The 2-tap difference filters in $\fftv{[1, -1]}$ and $\fftv{[1; -1]}$
are padded to size $(n \times n)$ before the FFT.
With $\mathbf{w}$ we can define a mapping between our 2D image space and
a rescaled FFT vector space:
\begin{equation}
\mathbf{z} = \mathbf{w} \circ \fftv{Z}
\end{equation}
Where $\circ$ is an element-wise product.
This mapping lets us rewrite the optimization problem in Eq.~\ref{eq:primal_loss} as:
\begin{equation}
\resizebox{2.9in}{!}{$
Z^* = \ifftv{ {1 \over \mathbf{w}} \left( \displaystyle \argmin_{\mathbf{z}} \left( f \left( \ifftv{ {\mathbf{z} \over \mathbf{w}}} \right) + \norm{\mathbf{z}}^2 \right)} \right)
\label{eq:fourier_loss}
$}
\end{equation}
where $\ifftv{\cdot}$ is the inverse of $\fftv{\cdot}$, and division is element-wise.
This reparametrization reduces the complicated regularization of $Z$
to a simple L2 regularization of $\mathbf{z}$, which has a preconditioning
effect.

\begin{figure}[t!]
\centering
  \stackunder[5pt]
    {\footnotesize Logistic Loss}
    {\includegraphics[width=1.6in]{figures/optimization_loss_1.png}}
  \stackunder[5pt]
    {\footnotesize BVM Loss}
    {\includegraphics[width=1.6in]{figures/optimization_loss_2.png}}
  \caption{
  Loss traces for our two stages of training, for three fold
  cross validation (each line represents a fold)
  on the Gehler-Shi dataset using LBFGS. Our preconditioned
  frequency domain optimization produces lower minima at greater rates
  than are achieved by non-preconditioned optimization in the
  frequency domain or naive optimization in the time domain.
  \label{fig:opt}
  }
\end{figure}

We use this technique during training to reparameterize all model
components $( \{F_k \}, G, B )$ as rescaled FFT vectors, each with their own
values for $\lambda_0$ and $\lambda_1$. The effect of this can be seen in
Fig.~\ref{fig:opt}, where we show the loss during our two training stages.
We compare against naive time-domain optimization (Eq.~\ref{eq:primal_loss})
and non-preconditioned frequency-domain optimization (Eq.~\ref{eq:fourier_loss}
with $\mathbf{w}=1$).
Our preconditioned reformulation exhibits a significant speedup and finds
minima with lower losses.

For all experiments (excluding our ``deep'' variants, see the supplement),
training is as follows:
All model parameters are initialized to $0$,
then we have a convex pre-training step which optimizes
Eq.~\ref{eq:fourier_loss} where $f(\cdot)$ is a logistic loss
(described in the supplement) using LBFGS for $16$ iterations,
and then we optimize Eq.~\ref{eq:fourier_loss}
where $f(\cdot)$ is the non-convex BVM loss in Eq.~\ref{eq:lossdata} using
LBFGS for $64$ iterations.

\section{Temporal Smoothing}
\label{sec:temporal}

Color constancy is usually studied in the context of
individual images, which are assumed to be IID.
But a practical white balance algorithm must run on a video sequence,
and must enforce some temporal smoothing of the predicted illuminant
to avoid presenting the viewer with an erratically-varying image in the viewfinder.
This smoothing cannot be too aggressive or else the viewfinder may appear
unresponsive when the illumination changes rapidly
(a colorful light turning on, the camera quickly moving outdoors, etc).
Additionally, when faced with multiple valid hypotheses
(a blue wall under white light vs a white wall under blue light, etc) we
may want to use earlier images to resolve ambiguities.
These desiderata of stability, responsiveness, and robustness are at odds
with each other, and so some compromise must be struck.

Our task of constructing a temporally coherent illuminant estimate is aided
by the probabilistic nature of the output of our per-frame model, which
produces a posterior distribution over illuminants parametrized as a bivariate
Gaussian.
Let us assume that we have some ongoing estimate of the illuminant and its
covariance $(\boldsymbol{\mu}_t, \boldsymbol{\Sigma}_t)$.
Given the observed mean and covariance $(\boldsymbol{\mu}_o, \boldsymbol{\Sigma}_o)$
provided by our model we update our
ongoing estimate by first convolving it with an zero-mean isotropic Gaussian
(encoding our prior belief that the illuminant may change over time)
and then multiplying that ``fuzzed'' Gaussian by the observed Gaussian:
\begin{align}
\boldsymbol{\Sigma}_{t+1} &= \left( \left( \boldsymbol{\Sigma}_t + \begin{bmatrix}\alpha&0\\0&\alpha\end{bmatrix} \right)^{-1} + \boldsymbol{\Sigma}_o \right)^{-1} \\
\boldsymbol{\mu}_{t+1} &= \boldsymbol{\Sigma}_{t+1} \left( \left( \boldsymbol{\Sigma}_t + \begin{bmatrix}\alpha&0\\0&\alpha\end{bmatrix} \right)^{-1} \boldsymbol{\mu}_t + \boldsymbol{\Sigma}_o \boldsymbol{\mu}_o \right)^{-1} \nonumber
\end{align}
Where $\alpha$ is a parameter that defines the expected variance of the
illuminant over time.
This update resembles a Kalman filter but with a simplified transition model,
no control model, and variable observation noise.

This temporal smoothing is not used in our benchmarks, but
its effect can be seen in the supplemental video.

\section{Results}
\label{sec:results}

We evaluate our technique using two standard color constancy datasets:
the Gehler-Shi dataset \cite{Gehler08,shifunt}
and the Cheng \etal\ dataset \cite{Cheng14} (see Tables~\ref{table:gehler_shi}
and~\ref{table:cheng}).
For the Gehler-Shi dataset we present several ablations and variants
of our model to show the effect of each design decision
and to investigate trade-offs between speed and accuracy.
Models labeled ``full'' were run on $384 \times 256$ $16$-bit images,
while models labeled ``thumb'' were run on $48 \times 32$ $8$-bit images,
which are the kind of images that
a practical white-balance system embedded on a hardware device might use.
Models labeled ``4 channel'' use the four feature channels used in
\cite{BarronICCV2015}, while models labeled ``2 channel'' use the two
channels we present in Section~\ref{sec:extensions}.
We also present models in which we only use the ``pixel channel'' $I$
or the ``edge channel'' $E$ as input.
All models have a histogram size of $n=64$ except for Models K and L where
$n$ is varied to show the impact of illuminant aliasing.
Two models use ``gray world'' de-aliasing, and the rest use ``gray light''
de-aliasing.
The former seems slightly less effective than the latter unless chroma histograms are heavily aliased, which is why we use it in Model K.
Model C only has one training stage that minimizes logistic loss for $64$ iterations,
thereby removing the BVM fitting from training.
Model E fixes $G(i,j) = 1$ and $B(i,j) = 0$, thereby removing
the model's ability to reason about the absolute color of the illuminant.
Model B was trained only to minimize the data term
(ie, $\lambda_0 = \lambda_1 = 0$ in Eq.~\ref{eq:reg}) while
Model D uses L2 regularization but not total variation
(ie, $\lambda_1 = 0$ in Eq.~\ref{eq:reg}).
Models N, O and P are variants of Model J in which, instead of learning a fixed model $( \{ F_k \}, G, B )$
we express those model parameters as the output of a small 2-layer neural network.
As inputs to this network we use image metadata, which allows the model to reason about exposure time and camera sensor type,
and/or a CNN-produced feature vector \cite{Wang2014}, which allows the model to reason about semantics (see the supplement for details).
For each experiment we tune all $\lambda$ hyperparameters to minimize the
``average'' error during cross-validation, using cyclic coordinate descent.

Model P achieves the lowest-error results,
with a $20\%$ reduction in error on Gehler-Shi compared to the previously best-performing published technique.
This improvement in accuracy also comes with a significant speedup compared to
previous techniques: $\sim\!30$ ms/image for most models, compared to the $520$ ms of CCC
\cite{BarronICCV2015} or the $3$ seconds (on a GPU) of Shi \etal\ \cite{ShiECCV2016}.
Model Q (our fastest model) has an accuracy comparable to
\cite{BarronICCV2015} and \cite{ShiECCV2016} but takes only $1.1$ milliseconds
to process an image, making it hundreds or millions of times faster than the
current state-of-the art.
Additionally, our model appears to be faster to train than the state-of-the-art,
though training times for prior work are often not available.
All runtimes in Table~\ref{table:gehler_shi} for our model were computed on an
Intel Xeon CPU E5-2680.
Runtimes for the ``full'' model were produced using a
Matlab implementation, while runtimes for the ``thumb'' model were produced
using a Halide \cite{RaganKelley2012} CPU implementation
(our Matlab implementation of Model Q takes $2.37$ ms/image).
Runtimes for our ``+semantic'' models are not presented as we were unable to
profile \cite{Wang2014} accurately (CNN feature computation appears to dominate
runtime).

To demonstrate that our model is a viable automatic white
balance system for consumer photography, we ran our Halide code on a
2016 Google Pixel XL using the thumbnail images
computed by the device's camera stack.
This implementation ran at $1.44$ms per image, which is equivalent
to $30$ frames per second using $< 5\%$ of the total compute
budget, thereby satisfying our previously-stated speed requirements.
A video of our system running in real-time on a phone can be found in the
supplement.

\begin{table}[!]
\begin{center}
\resizebox{3.25in}{!}{
\Huge
\begin{tabular}{ l  | c c c c c | c || c c}
\multirow{2}{*}{Algorithm} & \multirow{2}{*}{Mean} & \multirow{2}{*}{Med.} & \multirow{2}{*}{Tri.} & Best & Worst & \multirow{2}{*}{Avg.} & Test & Train \\
 & & & & 25\% & 25\% & & Time & Time \\
\hline
\input{gehler_shi_table.tex}
\end{tabular}
}
\vspace{1mm}
\caption{
Performance on the Gehler-Shi dataset \cite{Gehler08,shifunt}.
We present five error metrics and their average
(the geometric mean) with the lowest error per metric
highlighted in yellow.
We present the time (in seconds) for training each model and for
evaluating a single image, when available.
\label{table:gehler_shi}}
\vspace{5mm}
\resizebox{3.25in}{!}{
\Huge
\begin{tabular}{ l  | c c c c c | c}
\multirow{2}{*}{Algorithm} & \multirow{2}{*}{Mean} & \multirow{2}{*}{Med.} & \multirow{2}{*}{Tri.} & Best & Worst & \multirow{2}{*}{Avg.} \\
 & & & & 25\% & 25\% &  \\
\hline
\input{cheng_table.tex}
\end{tabular}
}
\vspace{1mm}
\caption{
 Performance on the dataset from Cheng \etal \cite{Cheng14}, in the same format as Table~\ref{table:gehler_shi}, excluding runtimes. As was done in \cite{BarronICCV2015} we present the
 average performance (the geometric mean) over all $8$ cameras in the dataset.
\label{table:cheng}}
\end{center}
\end{table}

\section{Conclusion}

We have presented FFCC, a color constancy algorithm that produces a $13-20\%$
reduction in error and a $250-3000\times$ speedup relative to prior work.
In doing so we have introduced the concept of convolutional color
constancy on a torus, and we have introduced techniques for illuminant de-aliasing and
differentiable bivariate von Mises fitting required for this toroidal approach.
We have also presented a novel technique for fast Fourier-domain
optimization subject to a certain family of regularizers.
FFCC produces a complete posterior distribution over illuminants,
which lets us assess the model's confidence and also enables a Kalman
filter-like temporal smoothing model.
FFCC's speed, accuracy, and temporal consistency allows it to be
used for real-time white balance on a consumer camera.

\clearpage

{\small
\bibliographystyle{ieee}
\bibliography{fccc}
}


\end{document}
